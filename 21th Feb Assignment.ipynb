{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186f00b0-bcc8-40ab-b999-2837f26a23be",
   "metadata": {},
   "source": [
    "#### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab40c8-2070-4966-b1e0-beb2aeb472d8",
   "metadata": {},
   "source": [
    "* Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. \n",
    "* These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "* Web scraping is used in a variety of digital businesses that rely on data harvesting. Legitimate use cases include: Search engine bots crawling a site, analyzing its content and then ranking it. Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
    "* Areas of use of Web Scrapping :\n",
    "\n",
    "1. **Price Monitoring** :\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. **Market Research** :\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. **News Monitoring** : \n",
    "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59547191-a981-4f02-bb37-489066cf4a93",
   "metadata": {},
   "source": [
    "***\n",
    "#### Q2. What are the different methods used for Web Scraping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477607-e141-4313-9861-7e8f9437ec11",
   "metadata": {},
   "source": [
    "Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers.\n",
    "\n",
    "You can have Self-built Web Scrapers but that requires advanced knowledge of programming. And if you want more features in your Web Scraper, then you need even more knowledge. On the other hand, pre-built Web Scrapers are previously created scrapers that you can download and run easily. These also have more advanced options that you can customize.\n",
    "\n",
    "Browser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers. But Software Web Scrapers don’t have these limitations as they can be downloaded and installed on your computer. These are more complex than Browser web scrapers, but they also have advanced features that are not limited by the scope of your browser.\n",
    "\n",
    "Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. Local Web Scrapers, on the other hand, run on your computer using local resources. So, if the Web scrapers require more CPU or RAM, then your computer will become slow and not be able to perform other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9b182-3b5d-4520-9137-777227522259",
   "metadata": {},
   "source": [
    "***\n",
    "#### Q3. What is Beautiful Soup? Why is it used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc842d-9333-48c8-9c22-68a7103b2361",
   "metadata": {},
   "source": [
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don’t have to think about encodings. This tool not only helps you scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports several third-party Python parsers like lxml or hml5lib.\n",
    "\n",
    "You can learn more about the full spectrum of its capabilities here: Beautiful Soup documentation.\n",
    "\n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8bcd4-53c1-4150-943b-2e955b064449",
   "metadata": {},
   "source": [
    "***\n",
    "#### Q4. Why is flask used in this Web Scraping project?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf38e23-86ea-4fd7-a6f6-bed535acc06d",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ad594-f504-43f5-b284-904f874a8aef",
   "metadata": {},
   "source": [
    "***\n",
    "#### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca3ac0-103f-4949-8f16-f9561a1db373",
   "metadata": {},
   "source": [
    "* AWS CodePipeline :\n",
    "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipeline. It allows users to build, test and deploy code into a test or production environment using either the AWS CLI or a clean UI configuration process within the Amazon Console.\n",
    "\n",
    "* AWS Elastic Beanstalk\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926e0a0-464a-4262-932a-8e372fd078b9",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
